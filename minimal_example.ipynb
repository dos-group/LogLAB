{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, WeightedRandomSampler, Dataset\n",
    "import copy\n",
    "\n",
    "from utils import *\n",
    "from tokenization import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/{}.csv.gz'\n",
    "file = 'spirit2_min'\n",
    "fault_window = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data generate Fault Windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39928/39928 [01:21<00:00, 489.11it/s]\n",
      "  0%|          | 1331/500000 [00:00<00:37, 13289.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    365444\n",
      "0    134556\n",
      "Name: Localize_Label, dtype: int64\n",
      "0    348620\n",
      "1    151380\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [00:40<00:00, 12440.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data: Generate fault time windows\n",
    "df = pd.read_csv(path.format(file), nrows=500000)\n",
    "    \n",
    "# set datetime and label data range\n",
    "df['Label'] = np.where(df['t'].values=='-',0,1)\n",
    "df['datetime'] = pd.to_datetime(df['Timestamp'].astype(int), unit='s')\n",
    "df = label_data(df, fault_window)\n",
    "    \n",
    "# print properties\n",
    "print(df['Localize_Label'].value_counts())\n",
    "print(df['Label'].value_counts())\n",
    "    \n",
    "# tokenize content\n",
    "tokenizer_input = df['Content']\n",
    "tokenizer_filter = r'(\\[.+?\\]|\\(0x.+?\\)|\\([0-9]\\))|:|,|\\s+|=|\\.|\\||\\/|\\{|\\}'\n",
    "tokenizer = RegexTokenizer(tokenizer_filter, trunc_num=True)\n",
    "tokenizer.fit(tokenizer_input)\n",
    "df['tokenized'] = tokenizer.tokenized\n",
    "    \n",
    "# save csv\n",
    "file_preprocessed = file + '_' + str(fault_window)\n",
    "df.to_csv(path.format(file_preprocessed),compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LogLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_lengths = {'tbird2_min':20, 'spirit2_min':16, 'BGL.log_min':12}\n",
    "transform_to_tensor = transforms.Lambda(lambda lst: torch.tensor(lst))\n",
    "\n",
    "device = 'cuda:0'\n",
    "lr = 0.0001\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spirit2_min_5000\n",
      "500000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 154553/500000 [00:00<00:00, 1545527.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [00:00<00:00, 1540804.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train model\n",
      "0.07242126854400001\n",
      "Running epoch 1 / 5\n",
      "Epoch Step: 0 / 3907 Loss: 33020.378906\n",
      "Epoch Step: 1000 / 3907 Loss: 21017546.607422\n",
      "Epoch Step: 2000 / 3907 Loss: 30754657.709961\n",
      "Epoch Step: 3000 / 3907 Loss: 34719310.249634\n",
      "loss: 9057.126889458888 cond: 0.01\n",
      "Running epoch 2 / 5\n",
      "Epoch Step: 0 / 3907 Loss: 7699.666016\n",
      "Epoch Step: 1000 / 3907 Loss: 4759599.335205\n",
      "Epoch Step: 2000 / 3907 Loss: 5800008.798828\n",
      "Epoch Step: 3000 / 3907 Loss: 6065384.238083\n",
      "loss: 1562.5648838079835 cond: 0.01\n",
      "Running epoch 3 / 5\n",
      "Epoch Step: 0 / 3907 Loss: 478.182251\n",
      "Epoch Step: 1000 / 3907 Loss: 209748.516571\n",
      "Epoch Step: 2000 / 3907 Loss: 266660.446375\n",
      "Epoch Step: 3000 / 3907 Loss: 283774.354085\n",
      "loss: 73.26288064267884 cond: 0.01\n",
      "Running epoch 4 / 5\n",
      "Epoch Step: 0 / 3907 Loss: 28.641010\n",
      "Epoch Step: 1000 / 3907 Loss: 12773.170361\n",
      "Epoch Step: 2000 / 3907 Loss: 15797.719233\n",
      "Epoch Step: 3000 / 3907 Loss: 16560.905820\n",
      "loss: 4.270569631660613 cond: 0.01\n",
      "Running epoch 5 / 5\n",
      "Epoch Step: 0 / 3907 Loss: 1.395911\n",
      "Epoch Step: 1000 / 3907 Loss: 865.907456\n",
      "Epoch Step: 2000 / 3907 Loss: 1211.717472\n",
      "Epoch Step: 3000 / 3907 Loss: 1369.649956\n",
      "loss: 0.36720376391472004 cond: 0.01\n"
     ]
    }
   ],
   "source": [
    "pad_len = pad_lengths[file]   \n",
    "file_preprocessed = file + '_' + str(fault_window)\n",
    "print(file_preprocessed)\n",
    "    \n",
    "df = pd.read_csv(path.format(file_preprocessed), converters={\"tokenized\": literal_eval})\n",
    "print(len(df))\n",
    "    \n",
    "train_x = df['tokenized']\n",
    "train_y = df['Localize_Label']\n",
    "dataset = TextTokenizationDataset(train_x, pad_len, labels=train_y, transforms=transform_to_tensor)\n",
    "    \n",
    "# count classes\n",
    "class_counts = [np.count_nonzero(train_y == 0),np.count_nonzero(train_y == 1)]\n",
    "weights = 1.0 / (torch.Tensor(class_counts))\n",
    "weights = [weights[int(v)] for v in train_y]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(df), replacement=False)\n",
    "data_loader_train = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    \n",
    "print('make model')\n",
    "# make model\n",
    "vocab_size = get_vocab_size(df['tokenized'])\n",
    "model = make_model(\n",
    "    src_vocab=vocab_size,\n",
    "    tgt_vocab=128,\n",
    "    n=2,\n",
    "    d_model=256,\n",
    "    d_ff=256, \n",
    "    dropout=0.1,\n",
    "    max_len=dataset.max_token_len\n",
    ")\n",
    "    \n",
    "print('train model')\n",
    "    \n",
    "# train model\n",
    "model_opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=0.00005)\n",
    "numerator = (class_counts[0]/(class_counts[0]+class_counts[1]))\n",
    "numerator = numerator * numerator\n",
    "print(numerator)\n",
    "\n",
    "loss_compute = SimpleLossCompute(model.generator, model_opt, is_test=False, numerator=numerator)\n",
    "train_model(model, data_loader_train, loss_compute, device, epochs=epochs, trained_condition=0.01)\n",
    "    \n",
    "# save model\n",
    "model_name = file_preprocessed + '_' + str(pad_len)\n",
    "save_model(model, name=model_name)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test LogLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spirit2_min_5000_16\n",
      "Epoch Step: 0 / 245 Loss: 0.063675\n",
      "Epoch Step: 100 / 245 Loss: 7.381588\n",
      "Epoch Step: 200 / 245 Loss: 7.802961\n"
     ]
    }
   ],
   "source": [
    "model_name = file_preprocessed + '_' + str(pad_len)\n",
    "print(model_name)\n",
    "\n",
    "df = pd.read_csv(path.format(file_preprocessed), converters={\"tokenized\": literal_eval})\n",
    "model = load_model(name=model_name)\n",
    "    \n",
    "test_x = df['tokenized']\n",
    "test_y = df['Label']    \n",
    "    \n",
    "transform_to_tensor = transforms.Lambda(lambda lst: torch.tensor(lst))\n",
    "dataset_test = TextTokenizationDataset(test_x, pad_len, labels=test_y, transforms=transform_to_tensor)\n",
    "sampler_test = SequentialSampler(dataset_test)\n",
    "data_loader_test = DataLoader(dataset_test, sampler=sampler_test, batch_size=2048)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "loss_compute = SimpleLossCompute(model.generator, None, is_test=True)\n",
    "distances = run_classification_test(data_loader_test, model, loss_compute, device=device, step_size=100)\n",
    "df['distances'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies: 151380\n",
      "Abnormal mean: 111.73679694371835\n",
      "Normal mean: 3.3296707541189905\n",
      "Threshold: 57.53323384891867\n",
      "\n",
      "True positives and false positives: \n",
      "1    151244\n",
      "Name: Label, dtype: int64\n",
      "True neagtives and false negatives: \n",
      "0    348620\n",
      "1       136\n",
      "Name: Label, dtype: int64\n",
      "F1-Score: 0.9995505974410489\n",
      "Precision: 1.0\n",
      "Recall: 0.9991015986259744\n",
      "Accuracy: 0.999728\n"
     ]
    }
   ],
   "source": [
    "a = df[df['Label']==1]\n",
    "n = df[df['Label']==0]\n",
    "\n",
    "th = (n['distances'].mean() + a['distances'].mean()) / 2\n",
    "\n",
    "\n",
    "print('Anomalies: {}'.format(len(a)))\n",
    "print('Abnormal mean: {}'.format(a['distances'].mean()))\n",
    "print('Normal mean: {}'.format(n['distances'].mean()))\n",
    "print('Threshold: {}'.format(th))\n",
    "print('')\n",
    "    \n",
    "    \n",
    "pred_a = df[df['distances'] > th]\n",
    "pred_n = df[df['distances'] <= th]\n",
    "vc_a = pred_a['Label'].value_counts()\n",
    "vc_n = pred_n['Label'].value_counts()\n",
    "print('True positives and false positives: ')\n",
    "print(vc_a)\n",
    "print('True neagtives and false negatives: ')\n",
    "print(vc_n)\n",
    "    \n",
    "\n",
    "df['Pred_Label'] = [int(x > th) for x in df['distances']]\n",
    "y_test = df['Label']\n",
    "predicted = df['Pred_Label']\n",
    "print('F1-Score: {}'.format(metrics.f1_score(y_test, predicted)))\n",
    "print('Precision: {}'.format(metrics.precision_score(y_test, predicted)))\n",
    "print('Recall: {}'.format(metrics.recall_score(y_test, predicted)))\n",
    "print('Accuracy: {}'.format(metrics.accuracy_score(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
